\chapter{Algorytm eNN}
\label{chap:enn}
\section{Podstawowa wersja algorytmu}
Algorytm kNN pomimo wielu swoich zalet ma także kilka wad, które w niektórych przypadkach mogą niekorzystnie wpływać na wyniki działania. Jednym z charakterystycznych problemów jest przypadek kiedy rozkłady gęstości poszczególnych klas nie są równe. W takiej sytuacji elementy klasy o większym rozkładzie gęstości przeważają nad elementami pozostałych klas na danym obszarze. Dla algorytmu większość najbliższych sąsiadów będzie pochodziła z dominującej klasy co może zakłócać poprawność działania.

Jako rozwinięcie algorytmu kNN został stworzony algorytm eNN (\textit{ang. Extended Nearest Neighbours}) \cite{haibo-he}. Zaproponowana metoda podczas klasyfikacji bierze pod uwagę nie tylko k najbliższych sąsiadów klasyfikowanego obiektu, ale również ich otoczenie. 

Dla uproszczenia opis algorytmu zostanie przeprowadzony dla przypadku klasyfikacji pomiędzy dwoma klasami. Uogólnienie algorytmu dla dowolnej ilości klas sprowadza się jedynie do większej złożoności obliczeniowej. Dla każdego z k najbliższych sąsiadów próbki ze zbioru testowego wyznaczana jest funkcja statystyczna T opisująca otoczenie w jakim się znajduje. Funkcja T wyznaczana jest według wzoru:

\begin{equation}
T_{i} = \frac{1}{n_i k} {\sum_{x \in S_i} \sum_{r=1}^{k} I_r (x,S \in (S_1 \cup S_2))} 
\end{equation}\\
gdzie $S_1$ i $S_2$ są zbiorami próbek należących odpowiednio do klas 1 i 2, k jest zdefiniowaną ilością najbliższych sąsiadów a $I_r$ dane jest wzorem:

\begin{equation}
I_r(x,S) =\left\{\begin{matrix}
1 &&	dla\ x \in S_i \wedge NN_{r}(x,S) \in S_i
\\
0 &&	w\ pozostałych\ przypadkach
\end{matrix}\right.
\end{equation}
Funkcja $I_r$ przyjmuje wartość 1 jeżeli próbka x i jej r-sąsiad należą do tej samej klasy, natomiast wartość 0 przyjmuje w pozostałych przypadkach.
Funkcja $T_i$ przyjmuje wartości z zakresu [0,1]. Im większa wartość $T_i$ tym więcej próbek tej samej klasy znajduje się w otoczeniu badanej próbki. Małe wartości oznaczają małą ilość próbek tej samej klasy.

Mając nową, niesklasyfikowaną próbkę, w kolejnych krokach zostaje przypisana do poszczególnych klas a następnie dla każdego przypadku wyznaczona zostaje wartość funkcji $T_i^j$ według wzoru:

\begin{equation}
T_{i}^{j} = \frac{1}{n_i^j k} {\sum_{x \in S_{i,j}'} \sum_{r=1}^{k} I_r (x,S' \in (S_1 \cup S_2 \cup {Z}))} 
\end{equation}\\
gdzie j oznacza klasę do której została przypisana próbka Z.
W rozważanym przypadku, gdy pod uwagę brane są dwie klasy otrzymujemy cztery wyniki $T_1^1, T_2^1, T_1^2\ oraz\ T_2^2$. Próbka Z zostaje zakwalifikowana zgodnie ze wzorem:

\begin{equation}
f_{ENN} = arg_{j\in1,2} max \sum_{i=1}^{2} T_i^j
\end{equation}
Powyższy wzór w przypadku gdy w zbiorze znajduje się N klas występuje w postaci:
\begin{equation}
f_{ENN} = arg_{j\in1,2,..,N} max \sum_{i=1}^{N} T_i^j
\end{equation}
Klasyfikacja próbki do odpowiedniej klasy odbywa się poprzez wybór maksymalnej wartości funkcji $T_i$ spośród wyznaczonych metodą iteracyjną wartości dla kolejnych klas.

\newpage
\section{Alternatywna wersja algorytmu}
Analizując podstawową wersję algorytmu eNN można zaobserwować konieczność wyznaczenia wartości $T_i^j$ dla każdej kolejnej klasyfikowanej próbki. Wiąże się to  ze wzrostem złożoności obliczeniowej co nie jest pożądane w przypadku dużych zbiorów testowych. Rozwiązaniem tego problemu jest równoważna wersja algorytmu eNN, która została przedstawiona w tym paragrafie. 

W rozważanej wersji algorytmu pod uwagę brane jest nie tylko kto znajduje się w zbiorze najbliższych sąsiadów klasyfikowanej próbki Z ale także które próbki ze zbioru uczącego biorą pod uwagę próbkę Z jako jednego z k najbliższych sąsiadów. W tym celu próbka Z zostaje przypisana kolejno do wszystkich klas. W każdym przypadku zostają wyznaczone wartości $\Delta n_i^j$ gdzie j jest klasą do której została przypisana próbka Z, natomiast i jest klasą dla której obliczana jest zmiana ilości sąsiadów tej samej klasy. W przypadku gdy i = j zliczana jest ilość próbek klasy i dla której wzrosła ilość sąsiadów należących do tej samej klasy. W przypadku gdy $i\ \neq j$ wyznaczana jest ilość próbek klasy i dla których zmalała ilość sąsiadów należących do tej samej klasy.

Dla wyznaczonych wartości $n_i^j$ klasyfikacja próbki Z realizowana jest zgodnie z zależnością:

\begin{equation}
f_{ENN} = arg_{j \in 1,2,..,N} max \begin{Bmatrix}
(\frac{\Delta n_i^j + k_j -kT_i}{(n_i+1)k})
\end{Bmatrix}_{i=j} - \sum_{i \neq j}^N \frac{\Delta n_i^j}{n_i k}
\end{equation}\\
gdzie k jest zdefiniowaną ilością najbliższych sąsiadów, $n_i$ jest ilością próbek należących do klasy i, a $k_i$ jest ilością najbliższych sąsiadów należących do klasy i.

Obie wersje algorytmu są równoważne, jednak w przypadku drugiej wersji na początku działania algorytmu można wyznaczyć wektor wartości $T_i$ a podczas klasyfikacji kolejnych próbek ograniczyć się do wyznaczenia zmian w otoczeniach k najbliższych sąsiadów próbki Z, dla kolejnych przypisań próbki Z do odpowiednich klas.
